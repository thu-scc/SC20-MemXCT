# Run scripts

## Assumption

These scripts expects datasets to be put into the same level as ReproducibilityChallenge directory:

```
..
├── ReproducibilityChallenge
│   ├── compile
│   ├── run
│   ├── ...
├── datasets
│   ├── ADS1_sinogram.bin
│   ├── ADS1_theta.bin
│   ├── ADS2_sinogram.bin
│   ├── ADS2_theta.bin
│   ├── ADS3_sinogram.bin
│   ├── ADS3_theta.bin
│   ├── ADS4_sinogram.bin
│   ├── ADS4_theta.bin
│   ├── CDS1_sinogram.bin
│   ├── CDS1_theta.bin
│   ├── CDS2_sinogram.bin
│   ├── CDS2_theta.bin
│   ├── CDS3_sinogram.bin
│   └── CDS3_theta.bin
├── ...
```

## Single CPU experiment

To run single CPU experiment, go to `single_cpu` directory. It contains the following scripts:

- collect.sh: Collect environment info, including Azure metadata, CPU and GPU models, software and hardware infomation.
- manual_run.cpu.sh: Run memxct.cpu.
- cpu_grid_search.py: Enumerate different combinations of parameters and run manaual_run.cpu.sh.

The `manual_run.cpu.sh` script expects five arguments:

1. NTHREAD: Number of OpenMP threads to use
2. DATA: Can be one of: ADS[1-4], CDS[1-3]
3. SPATSIZE/SPECSIZE: Tile size
4. PROJBLOCK/BACKBLOCK: Block size
5. PROJBUFF/BACKBUFF: Buffer size

For example, the following line:

```shell
./manual_run.cpu.sh 40 ADS2 256 128 64
```

...will run one MPI process with 40 OpenMP threads. It uses ADS2 dataset and uses 256 as tile size, 128 as block size and 64 as buffer size respectively.

We have run single_cpu tests under F16s_v2(Intel) and HB60rs(AMD) SKUs. Output is provided at `run/output/single_cpu/single_{amd,intel}`. Each directory contains:

- NTHREAD.DATA.SPATSIZE.SPECSIZE.PROJBLOCK.BACKBLOCK.PROJBUFF.BACKBUFF.bin: Reconstructed tomogram
- NTHREAD.DATA.SPATSIZE.SPECSIZE.PROJBLOCK.BACKBLOCK.PROJBUFF.BACKBUFF.out: Process output
- env-output.txt: Environment info generated by collect.sh
- stream_result.txt: STREAM memory bandwidth result generated by STREAM benchmark

We also use `cpu_grid_search.py` to search for optimal parameters:

```shell
python3 cpu_grid_search.py --script manual_run.cpu.sh --thread $THREAD --tile_size $TILE_SIZE
```

It will enumerate parameters and run `manual_run.cpu.sh` many times.

On F16s_v2(Intel) machine, we use 16 cores in one socket and on HB60rs(AMD) machine, we use 30 cores in one socket. Thus, we run one MPI process and 16/30 threads respectively.

Timings are reported by MemXCT itself and timing data is extracted from its text output by scripts under `figures/scripts`.

## Single GPU experiment

To run single GPU experiment, go to `single_gpu` directory. It's very similar to `single_cpu` directory, except:

1. There is an extra arg for `manual_run.gpu.sh`: `./manual_run.gpu.sh $GPU $NTHREAD $DATA $TILE_SIZE $BLOCK_SIZE $BUFFER_SIZE` where `GPU` can be one of: k80, p100 and v100.
2. The `gpu_grid_search.py` needs an extra argument `--gpu $GPU`
3. Output files are put under `single_k80`, `single_p100` and `single_v100` respectively.

We use NC24r_Promo for K80, NC24rs_v2 for P100 and NC24rs_v3 for V100. Environment info of these SKU is put under corresponding directories.

## Strong scaling on CPU

Scripts for strong scaling on CPU are put under `strong_scaling_on_cpu` directory.

There are three scripts:

- manual_run.intel.cpu.sh: Top level script, which uses mpirun to launch jobs in one, two and four nodes
- inner.sh: Wrapper script for mpirun, which setups parameters for MemXCT
- collect.sh: Environment info collection

The `manual_run.intel.cpu.sh` expects seven args:

1. CPU_PER_NODE: Number of CPU sockets per node
2. NTHREAD: Number of CPU cores per socket
3. HOSTFILE: A host file consisting of hostnames
4. DATA: One of ADS[1-4] or CDS[1-3]
5. SPATSIZE/SPECSIZE: Tile size
6. PROJBLOCK/BACKBLOCK: Block size
7. PROJBUFF/BACKBUFF: Buffer size

We use Slurm to allocate nodes in one group, and then use `mpi -hostfile hostfile` to distribute jobs to nodes manually instead of using `srun`.

In each node, we run $CPU_PER_NODE MPI processes and each MPI process has $NTHREAD OpenMP threads. We use F16s_v2 machine, so $CPU_PER_NODE is always 1 and $NTHREAD is 16. Intel MPI is used in this stage.

Output files are put under `run/output/strong_scaling_on_cpu` and files have the following naming convention:

- NNODE.NTHREAD.DATA.SPATSIZE.SPECSIZE.PROJBLOCK.BACKBLOCK.PROJBUFF.BACKBUFF.bin: Reconstructed tomogram
- NNODE.NTHREAD.DATA.SPATSIZE.SPECSIZE.PROJBLOCK.BACKBLOCK.PROJBUFF.BACKBUFF.out: Process output
- env-output.txt: Environment info generated by collect.sh

## Strong scaling on GPU

Scripts for strong scaling on GPU are put under `strong_scaling_on_cpu` directory. It is similar to strong scaling on CPU, but with these differences:

1. OpenMPI is used for MPI implementation
2. `manual_run.openmpi.sh` expects an argument for GPU model
3. Output file name is prepended by `$GPU` and `$NNODE` is replaced by `$NTASK`(equals to the number of gpu)

In this stage, we use NC24rs_v2 machine, which each contains 4 P100 GPU cards.Specifically, we use 1-to-1 mapping for MPI process to GPU, so 4 MPI processes are run in one node. The script uses mpirun to run on one, two and four nodes, using four, eight and sixteen P100 GPU cards respectively.

Environment info is put under `env-output.txt`.
